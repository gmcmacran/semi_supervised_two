---
format:
  gfm:
    html-math-method: webtex
jupyter: python3
---

## Repo Overview

For all supervised machine learning projects, computing an accurate label is critical. It is fairly common to have bespoke data feeds for each client. This can cause an inability to create labels for some clients. Semi-supervised machine learning is a path forward when the predictor variables can be created across many clients but the response variable is only computable for a subset.

In this repo, I test scikit-learn's semi-supervised method SelfTrainingClassifier by asking

-   Does the semi-supervised model perform better than only using the known labels?
-   How does AUC change as the proportion of unknown labels increases?
-   Does how the label is missing affect the method? Random vs systematic.

```{python}
#| include: false
import os
import numpy as np
import pandas as pd
from plotnine import ggplot, aes, labs, facet_wrap
from plotnine import scale_x_continuous, scale_y_continuous
from plotnine import geom_line, geom_point, geom_hline

os.chdir('S:/Python/projects/semi_supervised_two')
```

## Simulation Setup

Basic outline:

-   Step 1: Create data.
-   Step 2: Train a model using a semi-supervised method. Compute test AUC.
-   Step 3: Train a model only using the known labels. Compute test AUC.

For each iteration, only fifty thousand data points are created (labeled and unlabeled). This process is repeated varying the proportion of data with unknown labels. Each combination of settings is repeated 10 times and an average is computed to reduce variability.

The first few iteration look like
```{python}
#| echo: false
result = pd.read_csv('data/result.csv')
print(result.head())
```

```{python}
#| echo: false
result = result.groupby(['prop', 'missingPattern'], as_index=False).mean().drop(['b'], axis=1)
```
## Results

For this simulation, labels are missing at random. Semi-supervised methods helped a bit when 50% or less of the labels are missing. For more than 50%, it is better to only use known labels.

```{python}
#| echo: false
temp = result.loc[result['missingPattern'] == 'create_missing_at_random_data']
temp = temp.melt(id_vars=['prop', 'missingPattern'], value_vars=['AUC_semi', 'AUC_known'])
(
    ggplot(temp, aes(x = 'prop', y = 'value', color = 'variable'))
    + geom_line()
    + geom_point()
    + scale_x_continuous(breaks = np.arange(.05, 1.05, .10))
    + scale_y_continuous(breaks = np.arange(0, 1.01, .01))
    + labs(x = "Proportion of Unknown Labels", y = "Test AUC")
    + facet_wrap('~missingPattern')
)
```

For this simulation, only positive class's labels are missing at random. The negative label is always known. This time the semi-supervised method never helped. Sometimes it hurt performance.

```{python}
#| echo: false
temp = result.loc[result['missingPattern'] == 'create_missing_class_one_data']
temp = temp.melt(id_vars=['prop', 'missingPattern'], value_vars=['AUC_semi', 'AUC_known'])
(
    ggplot(temp, aes(x = 'prop', y = 'value', color = 'variable'))
    + geom_line()
    + geom_point()
    + scale_x_continuous(breaks = np.arange(.05, 1.05, .10))
    + scale_y_continuous(breaks = np.arange(0, 1.01, .01))
    + labs(x = "Proportion of Unknown Positive Labels", y = "Test AUC")
    + facet_wrap('~missingPattern')
)
``` 

SelfTrainingClassifier is not a silver bullet. Relative to only using known labels, its performance depends on how the labels are missing (random vs systematic) and how much of the data is missing a label.